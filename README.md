# ğŸ§  RAG-based Assistant for Compliance Queries

## ğŸ“ Problem Statement

Employees waste time searching through complex compliance documents, leading to delays and inconsistent answers. This project solves that by automating responses using a RAG-based assistant.

---

## âš™ï¸ Project Pipeline Overview

The project is divided into **four stages**:

---

### 1. Document Processing & Chunking

We prepare the input data by:
ğŸ“„ **Script:** `chunking_and_embeddings.py`
- Loading `.docx` files from a folder
- Merging all text content into a single `.parquet` file
- Splitting long texts into smaller overlapping chunks  
  - First by sentences, then by line breaks (`\n`) for fine-tuning  
  - Ensures that chunk size stays within the 512-token limit of the embedding model

ğŸ“ **Output:** Cleaned and chunked `.parquet` files saved in `data/chunks/`

---

### 2. Embedding Generation

ğŸ“„ **Script:** `chunking_and_embeddings.py`
- A base class `GetEmbsNPYBase` handles chunking and text preparation
- A subclass `GetEmbsNPY_Gigachat` initializes the GigaChat embedder and builds embeddings
- Alternative subclasses can be added for Hugging Face or other models
- Each documentâ€™s chunks are saved:
  - `.npy` files in `data/embs/` (embedding vectors)
  - `.parquet` files in `data/chunks/` (original chunk texts)

ğŸ’¡ Chunks are stored per file to stay below Qdrantâ€™s 32 MB upload limit.

---

### 3. Upload to Qdrant Vector Database

ğŸ“„ **Script:** `qdrant_database.py`  
ğŸ³ **Requirement:** Qdrant must be running locally via Docker

**Docker Setup:**
```bash
docker pull qdrant/qdrant
docker run -p 6333:6333 -v $(pwd):/qdrant/storage qdrant/qdrant
```
Initializes the collection if not yet present
Resumes indexing from the last record if it already exists
Uses COSINE similarity for matching

ğŸ“ Output: A Qdrant collection populated with chunk embeddings and metadata

---

### 4. Query Answering with Retrieval + LLM
ğŸ“„ Script: retriever.py
- `RetrievalQA` retrieves relevant documents for multiple reformulated versions of the user query
- `StuffDocumentsChain` formats the documents and queries into a prompt and sends it to the LLM
- `Talk2DB` orchestrates the full process, optionally using conversation history and returning the final answer

ğŸ’¡ Supports multi-turn dialogue and modular prompt handling.

---

## ğŸ“ Folder Structure
<pre> 
  .
â”œâ”€â”€ configs/ 
â”‚ â””â”€â”€ config.yml                       # Configuration file for model and database parameters 
â”œâ”€â”€ custom_modules/ 
â”‚ â”œâ”€â”€ init.py 
â”‚ â”œâ”€â”€ chunking_and_embeddings.py       # Chunking documents and generating embeddings 
â”‚ â”œâ”€â”€ qdrant_database.py               # Uploading and managing vectors in Qdrant 
â”‚ â””â”€â”€ retriever.py                     # Query handling and langchain LLM prompt engineering 
â”œâ”€â”€ data/ 
â”‚ â”œâ”€â”€ chunks/                          # Individual chunk .parquet files 
â”‚ â”œâ”€â”€ embs/                            # Individual .npy files with embeddings 
â”‚ â”œâ”€â”€ chunks.parquet                   # Combined chunk data 
â”‚ â”œâ”€â”€ embs.npy                         # Combined embeddings 
â”‚ â””â”€â”€ texts.parquet                    # Original processed text data 
â”œâ”€â”€ laws/                              # Raw .docx legal documents 
â”œâ”€â”€ README.md                          # Project overview and documentation 
â””â”€â”€  legal_RAG_pipeline.ipynb          # Main notebook to run the end-to-end pipeline
</pre>
  
---

## ğŸš€ Features

- Sentence- and paragraph-aware chunking with overlap  
- GigaChat-based or Hugging Face embedding support  
- Docker-based Qdrant vector store with auto-creation of collections  
- Conversational Langchain-baised LLM integration with prompt engineering  
- Optional support for dialogue history  
- Modular, reusable components for embedding, retrieval, and query handling  

---

## âœ… Final Goal

A structured query-answering pipeline that returns:

- âœ… Final answer generated by an LLM  
- âœ… Relevant document chunks  
- âœ… Optional conversation history support  

---

## ğŸ“ Author

**Artur Garipov**  
[LinkedIn](https://www.linkedin.com/in/artur-garipov-36037a319) | [GitHub](https://github.com/Artur-Gar)
