# Прототип RAG

Постановка задачи:
"""
Нужна автоматизированная LLM по генерации ответов на запросы работников в части комплаенс. Добавим, ее например, на страницу комплаенс.
Доп. ВНД по остальным частям комплаенс смогу договориться позднее. 
"""

Задача состоит из следующих этапов:
1) Обработка начальных текстов и их чанкинг на куски
2) Построение эмбеддингов чанков
3) Загрузка эмбеддингов в векторную базу данных
4) Настройка ретривера из базы данных + составление финального промпта и инициализация LLM для получения ответа на воспрос пользователя

Для 1) этапа:
 - выгружаем все документы из папки и объединяем их в parquet. Сохраняем

Для 2) этапа - код embeddings_and_database.py:
 - Для построения эмбеддингов в скрипте есть 2 класса. Родительский (GetEmbsNPYBase) - осуществляет чанкинг текстов и на него ссылается дочерний (GetEmbsNPY_Gigachat), который инициализирует нужный эмбеддер
 - Как альтернатива GetEmbsNPY_Gigachat - можно написать дочерний класс с эмбеддером с Hugging Face
 - Деление текста осуществляется по предложениям и затем по абзацам. Просто по абзацам/предложениям не получится для Gigachat, так как у него стоит ограничение в 512 токенов для эмбеддера, а в документах есть очень длинные предложения 
 - Далее отрезки текста объединяются в чанки нужного размера
 - Так как Qdrant не может загружать файлы больше 32 мб, то у эмбеддингов чанков каждого документа свой файл .npy (data/embs). То же для текста чанков .parquet (data/chunks)

Для 3) этапа - также код embeddings_and_database.py + используем докер:
  Докер:
    - сначала создаем контейнер командой: docker pull qdrant/qdrant
    - далее его локально запускаем с помощью команды: run -p 6333:6333 -v .:/qdrant/storage qdrant/qdrant
  Теперь класс PostEmbsQdrant:
    - проверяет, есть ли коллекция с таким-то названием. Если нет - создает ее и начинает нумерацию точек (то же, что и записи) с 0. Если да - продолжает нумерацию новых записей после старых
    - метрика схожести - COSINE

Для 4) этапа - код retriever.py:
  Класс StuffDocumentsChain:
    используется для объединения найденных документов из БД с запросами в строки, подстановки в промпт и отправки его в LLM
    - На вход получаем chain_input из RetrievalQA, т.е. словарь со всеми запросами и отобранными для них документами.
    - Далее объединяем в 1 строку все документы. То же для всех вариантов запросов.
    - Затем полученные строки вставляем в шаблон промпта через .format
    - В конце отправляем полученный промпт в генеративную LLM и возвращаем ее ответ

  Класс RetrievalQA:
    используется для сбора документов из базы данных, подходящих для разных версий запроса
    - на входе получаем список formulated_queries, которые являются переформулированными версиями оригинального вопроса original_query.
    - Для каждого из переформулированных запросов ищем релевантные документы.
    - Подаем все найденные документы и запросы в класс StuffDocumentsChain и возвращаем его результат
  
  Класс Talk2DB (сновной):
    используется для инициализации векторного хранилища в качестве ретривер, создания нескольких вариантов переформулирования начального вопроса и получения ответа на вопрос пользователя от LLM
    - в функции get_response (часть кода _formulate_queries) есть возможность задать историю диалога с llm. 
      По умолчанию history=[], если же есть есть диалог, то задавать в формате [(user1, bot1),(user2,bot2),...]
 

