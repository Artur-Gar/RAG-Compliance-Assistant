# 🧠 RAG-based Assistant for Compliance Queries

## 📝 Problem Statement

Employees waste time searching through complex compliance documents, leading to delays and inconsistent answers. This project solves that by automating responses using a RAG-based assistant.

---

## ⚙️ Project Pipeline Overview

The project is divided into **four stages**:

---

### 1. Document Processing & Chunking

We prepare the input data by:
📄 **Script:** `chunking_and_embeddings.py`
- Loading `.docx` files from a folder
- Merging all text content into a single `.parquet` file
- Splitting long texts into smaller overlapping chunks  
  - First by sentences, then by line breaks (`\n`) for fine-tuning  
  - Ensures that chunk size stays within the 512-token limit of the embedding model

📁 **Output:** Cleaned and chunked `.parquet` files saved in `data/chunks/`

---

### 2. Embedding Generation

📄 **Script:** `chunking_and_embeddings.py`
- A base class `GetEmbsNPYBase` handles chunking and text preparation
- A subclass `GetEmbsNPY_Gigachat` initializes the GigaChat embedder and builds embeddings
- Alternative subclasses can be added for Hugging Face or other models
- Each document’s chunks are saved:
  - `.npy` files in `data/embs/` (embedding vectors)
  - `.parquet` files in `data/chunks/` (original chunk texts)

💡 Chunks are stored per file to stay below Qdrant’s 32 MB upload limit.

---

### 3. Upload to Qdrant Vector Database

📄 **Script:** `qdrant_database.py`  
🐳 **Requirement:** Qdrant must be running locally via Docker

**Docker Setup:**
```bash
docker pull qdrant/qdrant
docker run -p 6333:6333 -v $(pwd):/qdrant/storage qdrant/qdrant
```
Initializes the collection if not yet present
Resumes indexing from the last record if it already exists
Uses COSINE similarity for matching

📁 Output: A Qdrant collection populated with chunk embeddings and metadata

---

### 4. Query Answering with Retrieval + LLM
📄 Script: retriever.py
- `RetrievalQA` retrieves relevant documents for multiple reformulated versions of the user query
- `StuffDocumentsChain` formats the documents and queries into a prompt and sends it to the LLM
- `Talk2DB` orchestrates the full process, optionally using conversation history and returning the final answer

💡 Supports multi-turn dialogue and modular prompt handling.

---

## 🚀 Features

- Sentence- and paragraph-aware chunking with overlap  
- GigaChat-based or Hugging Face embedding support  
- Docker-based Qdrant vector store with auto-creation of collections  
- Conversational Langchain-baised LLM integration with prompt engineering  
- Optional support for dialogue history  
- Modular, reusable components for embedding, retrieval, and query handling  

---

## ✅ Final Goal

A structured query-answering pipeline that returns:

- ✅ Final answer generated by an LLM  
- ✅ Relevant document chunks  
- ✅ Optional conversation history support  

---

## 📝 Author

**Artur Garipov**  
[LinkedIn](https://www.linkedin.com/in/artur-garipov-36037a319) | [GitHub](https://github.com/Artur-Gar)
